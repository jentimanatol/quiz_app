[
  {
    "question": "In computer vision, the Five Ws—'Who, What, When, Where, Why'—primarily frame what?",
    "options": [
      "A) The five stages of training a neural network",
      "B) A taxonomy of visual understanding tasks",
      "C) The main types of robotic actuators",
      "D) The steps of scientific peer review"
    ],
    "answer": "B"
  },
  {
    "question": "Which task most closely matches the 'Who' question?",
    "options": [
      "A) Object classification",
      "B) Face or identity recognition",
      "C) Event boundary detection",
      "D) Scene depth estimation"
    ],
    "answer": "B"
  },
  {
    "question": "The 'What' question most directly corresponds to:",
    "options": [
      "A) Object classification (what category is present)",
      "B) Optical flow estimation (how pixels move)",
      "C) Pose estimation (body keypoints)",
      "D) Sensor calibration"
    ],
    "answer": "A"
  },
  {
    "question": "The 'Where' question typically involves:",
    "options": [
      "A) Determining lighting conditions only",
      "B) Localizing objects and understanding scene layout",
      "C) Compressing images for transmission",
      "D) Removing adversarial noise"
    ],
    "answer": "B"
  },
  {
    "question": "The 'When' question in vision emphasizes:",
    "options": [
      "A) Hardware clock synchronization for cameras",
      "B) Temporal and event understanding over time",
      "C) Batch size selection during training",
      "D) The year a dataset was collected"
    ],
    "answer": "B"
  },
  {
    "question": "Why is the 'Why' question often the hardest among the Five Ws?",
    "options": [
      "A) It requires high-resolution sensors",
      "B) It depends on causal and commonsense reasoning beyond pixels",
      "C) It is strictly an unsupervised learning problem",
      "D) It only appears in synthetic benchmarks"
    ],
    "answer": "B"
  },
  {
    "question": "Which historical anecdote showed early optimism about solving vision quickly?",
    "options": [
      "A) The invention of the Turing Test",
      "B) The 1966 'Summer Vision Project' for undergraduates",
      "C) The creation of the first GPU",
      "D) The launch of ImageNet"
    ],
    "answer": "B"
  },
  {
    "question": "Which statement best contrasts human visual understanding with current AI systems?",
    "options": [
      "A) Humans rely exclusively on edges and textures",
      "B) Humans effortlessly integrate context and commonsense to interpret scenes",
      "C) Humans require millions of labeled images for basic recognition",
      "D) Humans are unaffected by visual illusions"
    ],
    "answer": "B"
  },
  {
    "question": "Which example illustrates a 'shortcut' learned by a vision model?",
    "options": [
      "A) Using snow in the background to predict 'wolf' vs 'husky'",
      "B) Using batch normalization to stabilize training",
      "C) Using dropout for regularization",
      "D) Using model pruning to reduce size"
    ],
    "answer": "A"
  },
  {
    "question": "Which task most directly addresses 'Who' in an image?",
    "options": [
      "A) Semantic segmentation",
      "B) Optical character recognition",
      "C) Face recognition or re-identification",
      "D) Stereo depth estimation"
    ],
    "answer": "C"
  },
  {
    "question": "What is a core challenge in mapping pixels to meaning?",
    "options": [
      "A) It can be solved by a single linear classifier",
      "B) Visual understanding often requires background knowledge not present in the image",
      "C) It only depends on GPU memory size",
      "D) The mapping is fixed across all cultures"
    ],
    "answer": "B"
  },
  {
    "question": "Which task aligns most with the 'Where' question?",
    "options": [
      "A) Predicting training loss after one epoch",
      "B) Estimating the number of layers in a network",
      "C) Determining bounding boxes or spatial relations among objects",
      "D) Measuring the carbon footprint of training"
    ],
    "answer": "C"
  },
  {
    "question": "Why can high accuracy on a dataset be misleading?",
    "options": [
      "A) Accuracy cannot be computed reliably",
      "B) Models may exploit dataset-specific artifacts rather than genuine understanding",
      "C) Datasets never contain enough classes",
      "D) Licensing restrictions hide errors"
    ],
    "answer": "B"
  },
  {
    "question": "Which problem commonly undermines model performance in new environments?",
    "options": [
      "A) Deterministic training",
      "B) Distribution shift between training and deployment",
      "C) Overabundance of labeled data",
      "D) Excessive interpretability"
    ],
    "answer": "B"
  },
  {
    "question": "How do classification and localization differ?",
    "options": [
      "A) Classification predicts categories; localization identifies positions/regions",
      "B) Localization is unsupervised while classification is supervised",
      "C) Classification needs video; localization needs text captions",
      "D) Localization is rarely used in practice"
    ],
    "answer": "A"
  },
  {
    "question": "Which example best fits a 'When' task?",
    "options": [
      "A) Distinguishing dawn from dusk based on lighting cues",
      "B) Detecting text in an image",
      "C) Estimating camera intrinsics",
      "D) Counting objects on a table"
    ],
    "answer": "A"
  },
  {
    "question": "Why is context crucial for robust recognition?",
    "options": [
      "A) Objects never appear without consistent backgrounds",
      "B) Context provides constraints that help disambiguate similar-looking objects and scenes",
      "C) Context allows models to ignore objects entirely",
      "D) Context is only relevant for synthetic datasets"
    ],
    "answer": "B"
  },
  {
    "question": "Which of the following is a typical 'What' benchmark task?",
    "options": [
      "A) Image classification into object categories",
      "B) Estimating camera intrinsics",
      "C) Calibrating display gamma",
      "D) Scheduling GPU jobs"
    ],
    "answer": "A"
  },
  {
    "question": "Why can people answer 'Why' questions about images more readily than current models?",
    "options": [
      "A) People memorize all possible object configurations",
      "B) People possess rich commonsense and causal models of the world",
      "C) People use exclusively bottom-up processing",
      "D) People ignore temporal information"
    ],
    "answer": "B"
  },
  {
    "question": "Which difficulty often affects visual datasets and labels?",
    "options": [
      "A) Inability to store images on disk",
      "B) Annotator disagreement and ambiguity of categories",
      "C) Lack of any public datasets",
      "D) Universal agreement on visual taxonomies"
    ],
    "answer": "B"
  },
  {
    "question": "Which scenario exemplifies the 'Where' question?",
    "options": [
      "A) Inferring that a photo was taken at an airport from visual cues",
      "B) Determining the species of a bird",
      "C) Recognizing a famous person’s face",
      "D) Explaining why a glass shattered"
    ],
    "answer": "A"
  },
  {
    "question": "What does the Five Ws framing suggest about visual understanding?",
    "options": [
      "A) It is solved by pixel-level edge detection alone",
      "B) It involves multiple intertwined subproblems beyond simple labeling",
      "C) It is purely symbolic rules applied to images",
      "D) It relies only on unsupervised learning methods"
    ],
    "answer": "B"
  },
  {
    "question": "What is a common limitation when models learn dataset shortcuts?",
    "options": [
      "A) They become too slow to be useful",
      "B) They fail to generalize when superficial cues change",
      "C) They use too much labeled data",
      "D) They overfit only to synthetic images"
    ],
    "answer": "B"
  },
  {
    "question": "Which kind of reasoning is especially important to answer 'Why' in images?",
    "options": [
      "A) Frequency-domain filtering",
      "B) Causal and narrative reasoning about events",
      "C) GPU scheduling heuristics",
      "D) Numerical optimization of loss functions"
    ],
    "answer": "B"
  },
  {
    "question": "Which example best captures a 'When' task in video understanding?",
    "options": [
      "A) Identifying the moment a goal is scored in a soccer match",
      "B) Detecting the presence of a ball",
      "C) Estimating the camera brand",
      "D) Classifying the stadium type"
    ],
    "answer": "A"
  },
  {
    "question": "Which factor often causes models to fail outside the lab?",
    "options": [
      "A) Excessive regularization",
      "B) Changes in lighting, viewpoint, or background",
      "C) Too much compute",
      "D) Too many training epochs"
    ],
    "answer": "B"
  },
  {
    "question": "'Who' tasks can extend beyond faces to include:",
    "options": [
      "A) Identifying camera models",
      "B) Re-identifying people across different cameras or times",
      "C) Measuring shutter speed",
      "D) Detecting JPEG compression levels"
    ],
    "answer": "B"
  },
  {
    "question": "What is a key message of the Five Ws framework for vision?",
    "options": [
      "A) Vision is solved by simple linear models",
      "B) True understanding integrates recognition, context, time, place, and causality",
      "C) More labeled data alone guarantees human-level performance",
      "D) Vision is unrelated to common sense"
    ],
    "answer": "B"
  },
  {
    "question": "Which critique best fits many current vision systems?",
    "options": [
      "A) Benchmarks automatically imply human-level understanding",
      "B) Systems often lack robust, general, commonsense interpretation of images",
      "C) Hardware speed is the only barrier to human-level perception",
      "D) The Five Ws are obsolete"
    ],
    "answer": "B"
  },
  {
    "question": "Which term describes models that latch onto background cues instead of object features?",
    "options": [
      "A) Data leakage",
      "B) Spurious correlations",
      "C) Hardware latency",
      "D) Deterministic sampling"
    ],
    "answer": "B"
  },
  {
    "question": "What core idea allows convolutional layers to use far fewer parameters than fully connected layers?",
    "options": [
      "A) Random initialization",
      "B) Weight sharing across spatial locations",
      "C) Using only binary weights",
      "D) Removing nonlinearities"
    ],
    "answer": "B"
  },
  {
    "question": "What does a learned convolutional filter typically detect in early layers?",
    "options": [
      "A) Entire object categories",
      "B) High-level semantics like 'dog playing'",
      "C) Simple visual patterns such as edges or textures",
      "D) Video frame timestamps"
    ],
    "answer": "C"
  },
  {
    "question": "What is the main role of pooling (or strided convolutions) in CNNs?",
    "options": [
      "A) Increase spatial resolution",
      "B) Provide translation invariance and reduce spatial dimensions",
      "C) Convert images to text",
      "D) Replace the need for training data"
    ],
    "answer": "B"
  },
  {
    "question": "Which dataset/competition was pivotal in popularizing modern deep CNNs?",
    "options": [
      "A) MNIST digit recognition",
      "B) CIFAR-10",
      "C) ImageNet Large Scale Visual Recognition Challenge",
      "D) COCO Captions"
    ],
    "answer": "C"
  },
  {
    "question": "Which model’s success dramatically reduced error rates and sparked renewed interest in deep CNNs?",
    "options": [
      "A) LeNet-5",
      "B) AlexNet",
      "C) Naive Bayes",
      "D) SVM with HOG features"
    ],
    "answer": "B"
  },
  {
    "question": "Which activation function helped deep CNNs train more effectively in early breakthroughs?",
    "options": [
      "A) Linear",
      "B) Sigmoid-only everywhere",
      "C) ReLU",
      "D) Softmax in hidden layers"
    ],
    "answer": "C"
  },
  {
    "question": "What is the standard output for an image classification CNN trained on ImageNet?",
    "options": [
      "A) A segmentation mask for each pixel",
      "B) A ranked list of class probabilities",
      "C) A set of bounding boxes only",
      "D) A text caption"
    ],
    "answer": "B"
  },
  {
    "question": "Why is data augmentation (e.g., random crops, flips) used in training CNNs?",
    "options": [
      "A) To replace the need for labels",
      "B) To make training deterministic",
      "C) To reduce overfitting by increasing effective data variety",
      "D) To avoid using GPUs"
    ],
    "answer": "C"
  },
  {
    "question": "What is a 'receptive field' in the context of CNNs?",
    "options": [
      "A) The total number of learnable parameters",
      "B) The portion of the input image that influences a unit’s activation",
      "C) The number of classes in the dataset",
      "D) The GPU memory required to train"
    ],
    "answer": "B"
  },
  {
    "question": "Which of the following best describes transfer learning with ImageNet CNNs?",
    "options": [
      "A) Training a network from scratch for every new task",
      "B) Using pretrained features and fine-tuning on a new dataset",
      "C) Converting CNNs into RNNs",
      "D) Training only on grayscale images"
    ],
    "answer": "B"
  },
  {
    "question": "Why do deeper CNN architectures often outperform shallower ones (up to a point)?",
    "options": [
      "A) They always have fewer parameters",
      "B) They learn hierarchical features from edges to object parts to objects",
      "C) They remove the need for nonlinear activations",
      "D) They do not require large datasets"
    ],
    "answer": "B"
  },
  {
    "question": "What is a common metric reported for ImageNet classification performance?",
    "options": [
      "A) BLEU score",
      "B) Top-1 and Top-5 accuracy",
      "C) ROUGE-L",
      "D) FID score"
    ],
    "answer": "B"
  },
  {
    "question": "Which technique helps reduce co-adaptation of neurons and combats overfitting in CNNs?",
    "options": [
      "A) Dropout",
      "B) Increasing batch size only",
      "C) Removing all pooling layers",
      "D) Using only 1x1 convolutions"
    ],
    "answer": "A"
  },
  {
    "question": "What risk arises when a CNN learns 'shortcuts' from training data?",
    "options": [
      "A) The model becomes uninterpretable by design",
      "B) It fails to generalize when superficial cues change",
      "C) It cannot be deployed on GPUs",
      "D) It cannot represent nonlinear functions"
    ],
    "answer": "B"
  },
  {
    "question": "Which option best illustrates dataset bias in large-scale image collections?",
    "options": [
      "A) Balanced representation across all object contexts",
      "B) Labels and images reflecting cultural and social stereotypes",
      "C) Images exclusively taken under lab conditions",
      "D) Constant lighting across the entire dataset"
    ],
    "answer": "B"
  },
  {
    "question": "What vulnerability shows that tiny, human-imperceptible perturbations can change a CNN’s prediction?",
    "options": [
      "A) Vanishing gradients",
      "B) Adversarial examples",
      "C) Mode collapse",
      "D) Catastrophic forgetting"
    ],
    "answer": "B"
  },
  {
    "question": "Which practice can make CNN predictions more robust at test time?",
    "options": [
      "A) Evaluating only on the training split",
      "B) Test-time augmentation or ensembling",
      "C) Removing validation sets",
      "D) Using fewer classes"
    ],
    "answer": "B"
  },
  {
    "question": "What is the function of the softmax layer in a classification CNN?",
    "options": [
      "A) Applies convolution to the input",
      "B) Normalizes logits into a probability distribution over classes",
      "C) Detects edges in the image",
      "D) Reduces spatial resolution"
    ],
    "answer": "B"
  },
  {
    "question": "Why is zero-padding often used in convolutional layers?",
    "options": [
      "A) To increase the number of parameters",
      "B) To preserve spatial dimensions and control receptive field growth",
      "C) To remove the need for pooling",
      "D) To convert images to frequency domain"
    ],
    "answer": "B"
  },
  {
    "question": "What common concern arises when evaluating models only on curated benchmarks like ImageNet?",
    "options": [
      "A) Benchmarks are too large to compute accuracy",
      "B) High scores may not translate to real-world robustness",
      "C) They never contain enough training examples",
      "D) They forbid the use of GPUs"
    ],
    "answer": "B"
  },
  {
    "question": "Which of the following best distinguishes classification from object detection?",
    "options": [
      "A) Detection assigns a single label to the whole image",
      "B) Classification requires bounding boxes for each object",
      "C) Detection localizes objects with boxes/masks in addition to labeling",
      "D) Classification uses only grayscale images"
    ],
    "answer": "C"
  },
  {
    "question": "What practical benefit made ImageNet pretraining widely useful across vision tasks?",
    "options": [
      "A) It eliminated the need for labels entirely",
      "B) It produced generic visual features transferable to new tasks with limited data",
      "C) It reduced model size to near zero",
      "D) It solved 3D reconstruction automatically"
    ],
    "answer": "B"
  },
  {
    "question": "Which element was critical to the early success of deep CNNs on ImageNet?",
    "options": [
      "A) Exclusive use of CPU clusters",
      "B) Large labeled datasets plus GPU acceleration",
      "C) Replacing convolution with decision trees",
      "D) Training without any regularization"
    ],
    "answer": "B"
  },
  {
    "question": "What does stride control in a convolutional layer?",
    "options": [
      "A) The number of layers in the network",
      "B) The step size of the filter as it moves across the input",
      "C) The number of output classes",
      "D) The learning rate schedule"
    ],
    "answer": "B"
  },
  {
    "question": "Which statement best describes class activation/attention maps used with CNNs?",
    "options": [
      "A) They deterministically prove causal understanding",
      "B) They highlight image regions most influential for a prediction",
      "C) They always remove bias from data",
      "D) They replace the need for labels"
    ],
    "answer": "B"
  },
  {
    "question": "What issue can arise from mislabeled or offensive categories in large image datasets?",
    "options": [
      "A) Faster convergence",
      "B) Ethical and fairness concerns in downstream systems",
      "C) Guaranteed better generalization",
      "D) Automatic removal of spurious correlations"
    ],
    "answer": "B"
  },
  {
    "question": "Why can distribution shift particularly hurt CNN performance?",
    "options": [
      "A) CNNs only work on grayscale images",
      "B) Learned features may depend on contexts not present in new environments",
      "C) CNNs cannot be trained on large datasets",
      "D) CNNs ignore local patterns entirely"
    ],
    "answer": "B"
  },
  {
    "question": "Which description fits 'Top-5 accuracy' in ImageNet evaluation?",
    "options": [
      "A) The model must predict the exact class in its top-1 guess",
      "B) The correct class must appear among the model’s five highest-probability guesses",
      "C) The model predicts five classes for each image and must get all five correct",
      "D) The model predicts only five total classes across the dataset"
    ],
    "answer": "B"
  },
  {
    "question": "Which configuration typically appears near the end of a classification CNN?",
    "options": [
      "A) Convolution followed by softmax probabilities over classes",
      "B) K-means clustering",
      "C) Recurrent cells for temporal modeling",
      "D) A purely linear pipeline without activations"
    ],
    "answer": "A"
  },
  {
    "question": "What is a reasonable caution when interpreting spectacular benchmark results for CNNs?",
    "options": [
      "A) They guarantee broad human-level understanding",
      "B) They may reflect overfitting to dataset quirks or shortcuts",
      "C) They imply adversarial robustness",
      "D) They prove causal reasoning about scenes"
    ],
    "answer": "B"
  },
  {
    "question": "What is the primary goal of a learning algorithm in supervised machine learning?",
    "options": [
      "A) Memorize the training set",
      "B) Minimize training loss while generalizing well to unseen data",
      "C) Maximize the number of model parameters",
      "D) Always achieve 100% training accuracy"
    ],
    "answer": "B"
  },
  {
    "question": "Which split is used to tune hyperparameters without biasing the final evaluation?",
    "options": [
      "A) Training set",
      "B) Validation set",
      "C) Test set",
      "D) Unlabeled set"
    ],
    "answer": "B"
  },
  {
    "question": "A model fits the training data perfectly but performs poorly on new data. This is:",
    "options": [
      "A) Underfitting",
      "B) Good generalization",
      "C) Overfitting",
      "D) Regularization"
    ],
    "answer": "C"
  },
  {
    "question": "Which technique directly combats overfitting?",
    "options": [
      "A) Increasing model depth without constraint",
      "B) Adding L2 weight decay",
      "C) Using a larger learning rate",
      "D) Removing the validation set"
    ],
    "answer": "B"
  },
  {
    "question": "Early stopping works by:",
    "options": [
      "A) Halting training when training loss first reaches zero",
      "B) Halting training when validation performance stops improving",
      "C) Halting training at a fixed number of epochs regardless of performance",
      "D) Restarting training from random weights periodically"
    ],
    "answer": "B"
  },
  {
    "question": "Which combination characterizes the bias–variance trade-off?",
    "options": [
      "A) High bias, low variance → complex model that overfits",
      "B) Low bias, high variance → simple model that underfits",
      "C) High bias, low variance → simple model that underfits",
      "D) Low bias, low variance → simple model that underfits"
    ],
    "answer": "C"
  },
  {
    "question": "Cross-validation helps primarily with:",
    "options": [
      "A) Generating synthetic labels",
      "B) Measuring a model’s robustness across different training/validation splits",
      "C) Guaranteeing zero generalization error",
      "D) Preventing data preprocessing"
    ],
    "answer": "B"
  },
  {
    "question": "Data leakage occurs when:",
    "options": [
      "A) The model uses only the training set",
      "B) Information from the test set influences model training or hyperparameter choices",
      "C) The learning rate is too small",
      "D) The dataset contains class imbalance"
    ],
    "answer": "B"
  },
  {
    "question": "Which practice is safest to avoid leakage in preprocessing?",
    "options": [
      "A) Fit scalers on the entire dataset, then split",
      "B) Fit scalers on training data only and apply to validation/test",
      "C) Do not scale features at all",
      "D) Shuffle only the test set"
    ],
    "answer": "B"
  },
  {
    "question": "A model with high training error and high test error is likely:",
    "options": [
      "A) Overfitting",
      "B) Underfitting",
      "C) Well-regularized",
      "D) Experiencing label shift"
    ],
    "answer": "B"
  },
  {
    "question": "Which metric is most appropriate when positive and negative classes are highly imbalanced?",
    "options": [
      "A) Overall accuracy only",
      "B) Precision, recall, and F1-score",
      "C) Training loss",
      "D) Parameter count"
    ],
    "answer": "B"
  },
  {
    "question": "Calibration of a classifier means:",
    "options": [
      "A) Probabilities reflect true likelihoods of correctness",
      "B) The confusion matrix is diagonal",
      "C) The model never overfits",
      "D) Hyperparameters are tuned on the test set"
    ],
    "answer": "A"
  },
  {
    "question": "Which plot helps diagnose whether a model’s predicted probabilities are well-calibrated?",
    "options": [
      "A) Learning curve",
      "B) Reliability (calibration) curve",
      "C) ROC curve only",
      "D) t-SNE plot"
    ],
    "answer": "B"
  },
  {
    "question": "A learning curve showing both training and validation error decreasing and then stabilizing indicates:",
    "options": [
      "A) Severe overfitting throughout",
      "B) Training failure due to vanishing gradients",
      "C) Healthy learning; additional data may still improve performance",
      "D) That the model ignores the training set"
    ],
    "answer": "C"
  },
  {
    "question": "Which approach addresses class imbalance during training?",
    "options": [
      "A) Ignoring minority classes",
      "B) Oversampling minority classes or using class-weighted loss",
      "C) Reducing the training set size",
      "D) Eliminating the validation set"
    ],
    "answer": "B"
  },
  {
    "question": "Regularization primarily aims to:",
    "options": [
      "A) Increase training error only",
      "B) Reduce variance and improve generalization",
      "C) Eliminate the need for validation data",
      "D) Force the model to memorize labels"
    ],
    "answer": "B"
  },
  {
    "question": "Which statement about gradient descent is accurate?",
    "options": [
      "A) It updates parameters to minimize a loss function using gradients",
      "B) It randomly sets parameters at the end of training",
      "C) It increases the loss function to avoid local minima",
      "D) It requires zero learning rate to converge"
    ],
    "answer": "A"
  },
  {
    "question": "Feature scaling (e.g., standardization) is important because:",
    "options": [
      "A) It guarantees perfect accuracy",
      "B) It ensures all models are unbiased",
      "C) It can speed up convergence and stabilize training",
      "D) It removes the need for regularization"
    ],
    "answer": "C"
  },
  {
    "question": "AUC-ROC primarily measures:",
    "options": [
      "A) The expected training time",
      "B) Trade-off between true positive rate and false positive rate across thresholds",
      "C) Correlation between features",
      "D) Number of classes"
    ],
    "answer": "B"
  },
  {
    "question": "Which is a typical signal of distribution shift harming performance?",
    "options": [
      "A) Training accuracy increases, test accuracy stable",
      "B) Similar metrics across validation and test sets",
      "C) Good validation performance but poor performance in deployment",
      "D) Test loss and validation loss identical"
    ],
    "answer": "C"
  },
  {
    "question": "Which tactic helps distinguish whether more data would likely improve a model?",
    "options": [
      "A) Examine learning curves",
      "B) Use a higher learning rate",
      "C) Add more layers without evaluation",
      "D) Lower input resolution"
    ],
    "answer": "A"
  },
  {
    "question": "Hyperparameters are typically:",
    "options": [
      "A) Learned by gradient descent automatically",
      "B) Fixed values or schedules chosen via validation or search strategies",
      "C) Equivalent to model weights",
      "D) Always the same across tasks"
    ],
    "answer": "B"
  },
  {
    "question": "Which practice is a sound final model evaluation protocol?",
    "options": [
      "A) Tune on the test set for best possible score",
      "B) Freeze hyperparameters after validation, then evaluate once on the test set",
      "C) Merge validation and test sets for more data",
      "D) Report only training loss"
    ],
    "answer": "B"
  },
  {
    "question": "Which description best matches underfitting?",
    "options": [
      "A) The model is too simple to capture patterns in the data",
      "B) The model is too complex and memorizes training data",
      "C) The model perfectly calibrates probabilities",
      "D) The model uses the test set to tune parameters"
    ],
    "answer": "A"
  },
  {
    "question": "Which diagnostic helps spot label noise issues?",
    "options": [
      "A) Inspect misclassified examples and their annotations",
      "B) Increase batch size only",
      "C) Train for more epochs without evaluation",
      "D) Remove the training set"
    ],
    "answer": "A"
  },
  {
    "question": "Which method can improve interpretability of model decisions?",
    "options": [
      "A) Randomizing labels",
      "B) Using post-hoc explanations (e.g., feature importance, saliency/attribution)",
      "C) Hiding model outputs from users",
      "D) Ignoring domain knowledge"
    ],
    "answer": "B"
  },
  {
    "question": "A model that performs well on a narrow benchmark but fails in deployment most likely:",
    "options": [
      "A) Achieved true understanding",
      "B) Exploited shortcuts or spurious correlations in the benchmark",
      "C) Is perfectly calibrated",
      "D) Used too much regularization"
    ],
    "answer": "B"
  },
  {
    "question": "Which statement about the test set is correct?",
    "options": [
      "A) It is used to fit scalers and select features",
      "B) It should be used only once for final evaluation",
      "C) It is interchangeable with the training set",
      "D) It must be larger than the training set"
    ],
    "answer": "B"
  },
  {
    "question": "Which approach helps handle noisy labels during training?",
    "options": [
      "A) Ignore label quality entirely",
      "B) Robust loss functions or filtering/cleaning suspicious samples",
      "C) Tune hyperparameters on the test set",
      "D) Use no validation split"
    ],
    "answer": "B"
  },
  {
    "question": "Which scenario most clearly indicates data leakage?",
    "options": [
      "A) Very high training accuracy, moderate validation accuracy",
      "B) Validation features inadvertently include information computed from the full dataset",
      "C) Slow training despite many epochs",
      "D) High test accuracy after regularization"
    ],
    "answer": "B"
  },
  {
    "question": "What does a language model primarily learn to do?",
    "options": [
      "A) Parse images into objects",
      "B) Predict the next token or sequence probability",
      "C) Convert speech to MIDI notes",
      "D) Translate sentiment into numbers without text"
    ],
    "answer": "B"
  },
  {
    "question": "The distributional hypothesis (“you shall know a word by the company it keeps”) motivates:",
    "options": [
      "A) Rule-based parsers",
      "B) Word embeddings that capture co-occurrence patterns",
      "C) Optical character recognition",
      "D) Phoneme-to-grapheme models"
    ],
    "answer": "B"
  },
  {
    "question": "A core limitation of n-gram language models is:",
    "options": [
      "A) They cannot be trained on text",
      "B) Severe sparsity and poor long-range dependency modeling",
      "C) They require images as input",
      "D) They cannot compute probabilities"
    ],
    "answer": "B"
  },
  {
    "question": "Which neural architecture was introduced to better capture long-range dependencies in sequences?",
    "options": [
      "A) k-Nearest Neighbors",
      "B) LSTM/GRU with gating mechanisms",
      "C) Decision Trees",
      "D) Naive Bayes"
    ],
    "answer": "B"
  },
  {
    "question": "In sequence-to-sequence learning for machine translation, the typical components are:",
    "options": [
      "A) Encoder–decoder networks",
      "B) Discriminators only",
      "C) Autoencoders for images",
      "D) Graph neural networks for social graphs"
    ],
    "answer": "A"
  },
  {
    "question": "What is the main idea behind attention in neural translation systems?",
    "options": [
      "A) Ignore most of the source sentence",
      "B) Dynamically focus on relevant parts of the source when generating each target word",
      "C) Replace the decoder with a parser",
      "D) Use only word frequency counts"
    ],
    "answer": "B"
  },
  {
    "question": "Word embeddings such as word2vec or GloVe primarily capture:",
    "options": [
      "A) Exact dictionary definitions",
      "B) Semantic similarity via distributional co-occurrence",
      "C) Phonetic spellings",
      "D) Sentence boundary rules"
    ],
    "answer": "B"
  },
  {
    "question": "Which challenge involves choosing the correct meaning of a word with multiple senses?",
    "options": [
      "A) Part-of-speech tagging",
      "B) Word sense disambiguation",
      "C) Syntactic parsing",
      "D) Lemmatization"
    ],
    "answer": "B"
  },
  {
    "question": "Syntactic ambiguity (e.g., garden-path sentences) highlights difficulties in:",
    "options": [
      "A) Image segmentation",
      "B) Mapping between surface structure and intended meaning",
      "C) Phoneme recognition",
      "D) Byte-pair encoding"
    ],
    "answer": "B"
  },
  {
    "question": "A common metric used to evaluate machine translation quality is:",
    "options": [
      "A) Perplexity only",
      "B) BLEU score",
      "C) PSNR",
      "D) Intersection-over-Union"
    ],
    "answer": "B"
  },
  {
    "question": "Which issue often appears when language models generate fluent text?",
    "options": [
      "A) Guaranteed factual accuracy",
      "B) Hallucinations—plausible but incorrect statements",
      "C) Unlimited context length",
      "D) Perfect grounding to real-world knowledge"
    ],
    "answer": "B"
  },
  {
    "question": "Intrinsic evaluation in NLP refers to:",
    "options": [
      "A) Testing models inside robot bodies",
      "B) Assessing on proxy tasks (e.g., word similarity) rather than end tasks",
      "C) Only evaluating on human exams",
      "D) Ignoring metrics altogether"
    ],
    "answer": "B"
  },
  {
    "question": "Which is a typical drawback of BLEU as an evaluation metric?",
    "options": [
      "A) It cannot compare any two systems",
      "B) It may not fully reflect human judgments of adequacy and fluency",
      "C) It requires audio input",
      "D) It is only defined for single-word outputs"
    ],
    "answer": "B"
  },
  {
    "question": "Distributional semantics can struggle with:",
    "options": [
      "A) Capturing co-occurrence patterns",
      "B) Encoding visual features",
      "C) Compositional meaning, negation, and quantifiers",
      "D) Tokenization"
    ],
    "answer": "C"
  },
  {
    "question": "Which task focuses on identifying people, places, or organizations in text?",
    "options": [
      "A) Dependency parsing",
      "B) Named entity recognition",
      "C) Lemmatization",
      "D) Topic modeling"
    ],
    "answer": "B"
  },
  {
    "question": "Sentiment analysis often fails on:",
    "options": [
      "A) Long words only",
      "B) Sarcasm and subtle context cues",
      "C) Numeric tokens",
      "D) Short sentences"
    ],
    "answer": "B"
  },
  {
    "question": "A key benefit of pretraining followed by fine-tuning is:",
    "options": [
      "A) Models never overfit",
      "B) Transfer of general language features to specific tasks with less labeled data",
      "C) Elimination of tokenization",
      "D) Guaranteed interpretability"
    ],
    "answer": "B"
  },
  {
    "question": "Which approach helps with out-of-vocabulary words in neural NLP models?",
    "options": [
      "A) Fixed word lists only",
      "B) Subword tokenization (e.g., BPE) or character-level modeling",
      "C) Ignoring rare words",
      "D) Mapping all unknowns to a single token without change"
    ],
    "answer": "B"
  },
  {
    "question": "Coreference resolution aims to:",
    "options": [
      "A) Link mentions that refer to the same entity",
      "B) Count the number of nouns in a document",
      "C) Convert active to passive voice",
      "D) Remove stopwords"
    ],
    "answer": "A"
  },
  {
    "question": "Which problem exposes the need for commonsense knowledge in pronoun resolution?",
    "options": [
      "A) Edit distance",
      "B) Winograd-style schemas",
      "C) Byte-pair encoding",
      "D) Zipf’s law"
    ],
    "answer": "B"
  },
  {
    "question": "Domain shift in NLP typically appears when:",
    "options": [
      "A) Models are evaluated on handwriting",
      "B) Training on newswire and testing on social media slang",
      "C) Switching GPUs during training",
      "D) Using different optimizers"
    ],
    "answer": "B"
  },
  {
    "question": "Bias in language datasets can manifest as:",
    "options": [
      "A) Neutral word distributions",
      "B) Stereotyped associations and unequal performance across groups",
      "C) Deterministic tokenization",
      "D) Perfect fairness by default"
    ],
    "answer": "B"
  },
  {
    "question": "Compositional generalization refers to:",
    "options": [
      "A) Learning to combine known parts in novel ways",
      "B) Memorizing the training set",
      "C) Counting words accurately",
      "D) Removing punctuation"
    ],
    "answer": "A"
  },
  {
    "question": "A common objective for training language models on raw text is:",
    "options": [
      "A) Mean squared error on image pixels",
      "B) Next-token prediction (or masked-token prediction)",
      "C) Minimizing audio waveform distortion",
      "D) Maximizing BLEU directly"
    ],
    "answer": "B"
  },
  {
    "question": "Why do models that excel on narrow benchmarks sometimes fail in real use?",
    "options": [
      "A) Benchmarks are always harder than real-world tasks",
      "B) Overfitting to dataset quirks and shortcuts rather than real understanding",
      "C) Lack of enough parameters",
      "D) Using pretraining"
    ],
    "answer": "B"
  },
  {
    "question": "Which challenge is central to aligning language models with human expectations?",
    "options": [
      "A) Using fewer tokens",
      "B) Ensuring outputs are truthful, safe, and non-toxic",
      "C) Removing all training data",
      "D) Avoiding any human oversight"
    ],
    "answer": "B"
  },
  {
    "question": "Pragmatics in language understanding concerns:",
    "options": [
      "A) The physical layout of keyboards",
      "B) How context and speaker intent shape meaning beyond literal words",
      "C) Binary encoding of characters",
      "D) Only sentence boundary detection"
    ],
    "answer": "B"
  },
  {
    "question": "A typical sign that a model lacks grounding is:",
    "options": [
      "A) It refuses to process text",
      "B) Fluent output that contradicts real-world facts or common sense",
      "C) Perfectly calibrated probabilities",
      "D) Reliance on human feedback"
    ],
    "answer": "B"
  },
  {
    "question": "Perplexity is commonly used to evaluate language models because it:",
    "options": [
      "A) Measures image reconstruction error",
      "B) Reflects how well a model predicts a sample (lower is better)",
      "C) Proves human-level understanding",
      "D) Guarantees truthful outputs"
    ],
    "answer": "B"
  },
  {
    "question": "Which phenomenon with word embeddings showed vector arithmetic capturing analogies (e.g., king − man + woman ≈ queen)?",
    "options": [
      "A) Conjugation invariance",
      "B) Linear relational structure in embedding spaces",
      "C) Autoregressive decoding",
      "D) Byte-pair encoding"
    ],
    "answer": "B"
  }
]