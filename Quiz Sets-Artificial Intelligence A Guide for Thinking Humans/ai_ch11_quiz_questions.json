[
  {
    "question": "What is algorithmic bias most directly?",
    "options": [
      "A) Random variation in model outputs",
      "B) Systematic and repeatable errors that create unfair outcomes for certain groups",
      "C) Any difference in accuracy across models",
      "D) Using too many parameters in a network"
    ],
    "answer": "B"
  },
  {
    "question": "Which practice best reduces hidden bias entering from data collection?",
    "options": [
      "A) Use a single source that’s easy to access",
      "B) Document data provenance and sampling with standardized checklists",
      "C) Avoid describing data at all",
      "D) Only collect data from volunteers"
    ],
    "answer": "B"
  },
  {
    "question": "‘Fairness through unawareness’ (dropping protected attributes) often fails because:",
    "options": [
      "A) Models cannot learn without those attributes",
      "B) Other features act as proxies for protected attributes",
      "C) It violates probability theory",
      "D) It reduces model capacity too much"
    ],
    "answer": "B"
  },
  {
    "question": "Demographic parity requires:",
    "options": [
      "A) Equal false positive rates across groups",
      "B) Equal selection/positive prediction rates across groups",
      "C) Equal true positive rates across groups",
      "D) Equal calibration curves across groups"
    ],
    "answer": "B"
  },
  {
    "question": "Equal opportunity focuses on equality of:",
    "options": [
      "A) Positive prediction rates",
      "B) False positive rates",
      "C) True positive rates (TPR) across groups",
      "D) Model parameter counts"
    ],
    "answer": "C"
  },
  {
    "question": "Equalized odds requires equality of:",
    "options": [
      "A) TPR and FPR across groups",
      "B) Only TPR across groups",
      "C) Only FPR across groups",
      "D) Sample sizes"
    ],
    "answer": "A"
  },
  {
    "question": "Calibration across groups means:",
    "options": [
      "A) Predicted probabilities correspond to equal empirical frequencies within each group",
      "B) Each group has identical features",
      "C) The confusion matrices are identical",
      "D) All thresholds are the same"
    ],
    "answer": "A"
  },
  {
    "question": "Which statement reflects the 'impossibility' trade-offs in fairness?",
    "options": [
      "A) All fairness metrics can always be satisfied simultaneously",
      "B) Under different base rates, some fairness criteria are mutually incompatible",
      "C) Fairness only depends on model size",
      "D) Fairness is unrelated to thresholds"
    ],
    "answer": "B"
  },
  {
    "question": "Disparate impact is commonly assessed via:",
    "options": [
      "A) The 80% rule comparing selection rates",
      "B) The ROC AUC only",
      "C) The training loss value",
      "D) The number of model layers"
    ],
    "answer": "A"
  },
  {
    "question": "Measurement bias arises when:",
    "options": [
      "A) Features or labels contain systematic errors for certain groups",
      "B) The optimizer fails to converge",
      "C) Batch size is too small",
      "D) The dataset is too large"
    ],
    "answer": "A"
  },
  {
    "question": "A feedback loop in deployed models can:",
    "options": [
      "A) Eliminate bias automatically",
      "B) Amplify existing bias by influencing future data collection",
      "C) Freeze model parameters",
      "D) Guarantee equal error rates"
    ],
    "answer": "B"
  },
  {
    "question": "Which technique aims to explain a single prediction by approximating a local decision boundary?",
    "options": [
      "A) LIME",
      "B) K-means",
      "C) Beam search",
      "D) Byte-pair encoding"
    ],
    "answer": "A"
  },
  {
    "question": "SHAP values are used to:",
    "options": [
      "A) Visualize convolution filters",
      "B) Attribute a prediction to feature contributions using game-theoretic ideas",
      "C) Encode sentences as vectors",
      "D) Enforce demographic parity"
    ],
    "answer": "B"
  },
  {
    "question": "A 'model card' is best described as:",
    "options": [
      "A) A GPU driver",
      "B) A standardized report describing model intent, performance, and limitations",
      "C) A license key",
      "D) A learning rate schedule"
    ],
    "answer": "B"
  },
  {
    "question": "‘Datasheets for datasets’ primarily address:",
    "options": [
      "A) Hyperparameter tuning",
      "B) Systematic documentation of dataset motivation, composition, and collection",
      "C) Removing labels from data",
      "D) GPU memory allocation"
    ],
    "answer": "B"
  },
  {
    "question": "A high overall accuracy with large group disparities most likely indicates:",
    "options": [
      "A) Perfect fairness",
      "B) Fairness problems masked by aggregate metrics",
      "C) That calibration is perfect",
      "D) No need for auditing"
    ],
    "answer": "B"
  },
  {
    "question": "Which approach can reduce bias during training?",
    "options": [
      "A) Ignore minority classes",
      "B) Reweigh or resample to balance groups or use fairness-aware loss terms",
      "C) Remove the validation set",
      "D) Use only synthetic data"
    ],
    "answer": "B"
  },
  {
    "question": "Counterfactual fairness checks whether:",
    "options": [
      "A) Predictions remain the same when protected attributes are counterfactually changed",
      "B) ROC curves coincide",
      "C) The dataset is linearly separable",
      "D) The optimizer has low variance"
    ],
    "answer": "A"
  },
  {
    "question": "Which is a privacy-preserving technique often discussed with fairness?",
    "options": [
      "A) Differential privacy",
      "B) Increasing batch size",
      "C) Early stopping",
      "D) Data shuffling only"
    ],
    "answer": "A"
  },
  {
    "question": "A common ethical concern with predictive policing systems is that they:",
    "options": [
      "A) Decrease data collection",
      "B) Reinforce historical patterns and concentrate policing in certain communities",
      "C) Improve transparency by default",
      "D) Use only synthetic crimes"
    ],
    "answer": "B"
  },
  {
    "question": "Fairness audits should ideally be performed:",
    "options": [
      "A) Only once before deployment",
      "B) Continuously, including post-deployment monitoring",
      "C) Only after user complaints",
      "D) Only by the model developer"
    ],
    "answer": "B"
  },
  {
    "question": "Which concept allows individuals to understand and challenge automated decisions that affect them?",
    "options": [
      "A) Contestability and recourse",
      "B) Gradient descent",
      "C) Data augmentation",
      "D) Spectral clustering"
    ],
    "answer": "A"
  },
  {
    "question": "Proxy variables become problematic when they:",
    "options": [
      "A) Reduce training time",
      "B) Correlate with protected attributes and carry their effects into predictions",
      "C) Are standardized",
      "D) Are normalized to zero mean"
    ],
    "answer": "B"
  },
  {
    "question": "A risk with releasing detailed model internals is that it may:",
    "options": [
      "A) Always improve fairness",
      "B) Enable gaming or privacy leakage",
      "C) Remove dataset bias",
      "D) Eliminate the need for testing"
    ],
    "answer": "B"
  },
  {
    "question": "Group fairness focuses on metrics across groups, while individual fairness emphasizes:",
    "options": [
      "A) Equal treatment of similar individuals",
      "B) Identical class prevalences",
      "C) Matching parameter counts",
      "D) Equal training loss"
    ],
    "answer": "A"
  },
  {
    "question": "Which pipeline step most commonly introduces label bias?",
    "options": [
      "A) Optimizer selection",
      "B) Annotation practices and guidelines",
      "C) GPU type",
      "D) Batch size choice"
    ],
    "answer": "B"
  },
  {
    "question": "Why can threshold choice affect fairness outcomes?",
    "options": [
      "A) Thresholds never influence fairness",
      "B) Changing thresholds trades off TPR/FPR and can alter disparities",
      "C) Thresholds only affect training time",
      "D) Thresholds only matter for regression"
    ],
    "answer": "B"
  },
  {
    "question": "An effective governance program for responsible AI includes:",
    "options": [
      "A) Ad hoc decisions by one team",
      "B) Clear roles, impact assessments, documentation, and red-team testing",
      "C) Removing domain experts from the loop",
      "D) Ignoring user feedback"
    ],
    "answer": "B"
  },
  {
    "question": "Human oversight is crucial because:",
    "options": [
      "A) It guarantees there are no errors",
      "B) People can spot context-specific failures and intervene where automation struggles",
      "C) It eliminates the need for testing",
      "D) It replaces documentation"
    ],
    "answer": "B"
  },
  {
    "question": "A core caution when interpreting fairness metrics is that:",
    "options": [
      "A) One metric captures every notion of fairness",
      "B) Different metrics formalize different values and involve trade-offs",
      "C) Metrics are unrelated to societal goals",
      "D) Metrics always guarantee ethical outcomes"
    ],
    "answer": "B"
  }
]